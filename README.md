# CVPR2024_abaw

In this repository, we present a deep learning approach that functions similarly to a classical temporal convolution, integrating audio, visual, and textual modalities to perform frame-wise classification of ambivalence and hesitancy in videos.
We submitted our results on the test set for the Behavioural Ambivalence/Hesitancy (BAH) Recognition Challenge as part of the 8th ABAW competition.

The architecture builds upon our approach developed for the Emotional Mimicry Intensity (EMI) Estimation Challenge - another component of the 8th ABAW competition. The corresponding code is available in the EMI branch of this repository.

Use train_abaw.py to train the model and inference.py to generate predictions on the test data. The model in train_abaw.py can also be further customized.
